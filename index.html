<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Evgenii Zheltonozhskii </title> <meta name="author" content="Evgenii Zheltonozhskii"> <meta name="description" content="A personal website of Evgenii Zheltonozhskii. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://evgeniizh.com//"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social-links"> <a href="mailto:%7A%68%65%6C%74%6F%6E%6F%7A%68%73%6B%69%79@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://telegram.me/EvgeniyZh" title="telegram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-telegram"></i></a> <a href="https://orcid.org/0000-0002-5400-9321" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=1yHw4W0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Evgenii_Zheltonozhskii/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/Randl" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/zheltonozhskiy" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/evgeniyzhe" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a rel="me" href="https://mastodon.social/evgeniizh" title="Mastodon" target="_blank"><i class="fa-brands fa-mastodon"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Evgenii</span> Zheltonozhskii </h1> <p class="desc"><a href="https://www.technion.ac.il/en/home-2/" rel="external nofollow noopener" target="_blank">Technion – Israel Institute of Technology</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?83a56183fbbfc4da207433b42c8a785d" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Lidow Complex, room 309</p> <p>Technion</p> <p>Haifa, 3200003</p> </div> </div> <div class="clearfix"> <p>I’m a Physics Ph.D. student in Technion, advised by <a href="https://phsites.technion.ac.il/lindner/" rel="external nofollow noopener" target="_blank">Netanel Lindner</a> and supported by <a href="https://adams.academy.ac.il/" rel="external nofollow noopener" target="_blank">Adams fellowship</a>. You can find more details about my experience in short <a href="/assets/pdf/resume_zheltonozhskii.pdf">resume</a> or longer academic <a href="/assets/pdf/cv_zheltonozhskii.pdf">CV</a>.</p> <p><strong>Research interests</strong>: I’m interested in the condensed matter theory of strongly correlated materials and, in particular, topological phases and topological quantum computing, as well as the application of deep learning and self-supervised learning in physics. Currently, I focus on different aspects of topological quantum computing implementation. This includes experiments related to TQC, such as fractional quantum Hall edges, Kitaev spin liquid, and p+ip superconductors, possible realizations of universal quantum computing in topological systems, and usage of topological system simulations as quantum error-correction codes. I also have a <a href="https://t.me/j_links" rel="external nofollow noopener" target="_blank">Telegram channel</a> where I post links to research I find interesting.</p> <p><strong>Previously</strong>: I finished CS M.Sc. in Technion, advised by <a href="https://bron.cs.technion.ac.il/" rel="external nofollow noopener" target="_blank">Alex Bronstein</a>, Avi Mendelson, and <a href="https://chaimb.cs.technion.ac.il/" rel="external nofollow noopener" target="_blank">Chaim Baskin</a>, and studied reduced supervision in computer vision (in particular, self-supervised and semi-supervised learning). I was a research intern in Creative Vision team in Snap Research in Summer 2020, working on 3D reconstruction trained on single 2D views with <a href="https://ojwoodford.github.io/" rel="external nofollow noopener" target="_blank">Olly Woodford</a> and <a href="http://www.stulyakov.com/" rel="external nofollow noopener" target="_blank">Sergey Tulyakov</a>. Before that, I was part of <a href="https://excellence.technion.ac.il/en/" rel="external nofollow noopener" target="_blank">Rothschild Technion Program for Excellence</a> and received double B.Sc. (CS and Physics+Math, Cum Laude) from Technion. In 2017, I participated in Google Summer of Code under OpenCV organization.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May 11, 2023</th> <td> The paper “<a href="https://arxiv.org/abs/2305.06161" rel="external nofollow noopener" target="_blank">StarCoder: may the source be with you!</a>” was released to arxiv! This is result of awesome BigCode collaboration. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 27, 2023</th> <td> I was selected as one of the eight <a href="https://adams.academy.ac.il/congratulations-to-the-eight-new-adams-fellows-selected-for-the-academic-year-2023-2024/" rel="external nofollow noopener" target="_blank">Adams Fellows</a> for the 2023-2024 academic year. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 30, 2023</th> <td> My poster “Competition of Dissipative and Andreev Processes in Abelian Quantum Hall-Superconductor Junctions” was awarded 1st place prize at <a href="https://phys.technion.ac.il/en/research/research-day" rel="external nofollow noopener" target="_blank">Technion Physics Faculty Research day</a>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2abbr"><abbr class="badge">CVPR’22</abbr></div> <div id="botach2021mttr" class="col-sm-8"> <div class="title">End-to-End Referring Video Object Segmentation with Multimodal Transformers</div> <div class="author">AdamBotach, <em>EvgeniiZheltonozhskii</em>, and <a href="https://chaimb.cs.technion.ac.il/" rel="external nofollow noopener" target="_blank">ChaimBaskin</a> </div> <div class="periodical"> <em>In <span style="color:brown">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span></em>, Jun 2022</div> <div class="periodical"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">URL</a><a href="https://arxiv.org/abs/2111.14821" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a><a href="https://github.com/mttr2021/MTTR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The referring video object segmentation task (RVOS) involves segmentation of a text-referred object instance in the frames of a given video. Due to the complex nature of this multimodal task, which combines text reasoning, video understanding, instance segmentation and tracking, existing approaches typically rely on sophisticated pipelines in order to tackle it. In this paper, we propose a simple Transformer-based approach to RVOS. Our framework, termed Multimodal Tracking Transformer (MTTR), models the RVOS task as a sequence prediction problem. Following recent advancements in computer vision and natural language processing, MTTR is based on the realization that video and text can both be processed together effectively and elegantly by a single multimodal Transformer model. MTTR is end-to-end trainable, free of text-related inductive bias components and requires no additional mask-refinement post-processing steps. As such, it simplifies the RVOS pipeline considerably compared to existing methods. Evaluation on standard benchmarks reveals that MTTR significantly outperforms previous art across multiple metrics. In particular, MTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and JHMDB-Sentences datasets respectively, while processing 76 frames per second. In addition, we report strong results on the public validation set of Refer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the attention of researchers. The code to reproduce our experiments is available at this https URL</p> </div> <div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">botach2021mttr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{End-to-End Referring Video Object Segmentation with Multimodal Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Botach, Adam and Zheltonozhskii, Evgenii and Baskin, Chaim}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openaccess.thecvf.com/content/CVPR2022/html/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.html}</span><span class="p">,</span>
  <span class="na">sortkey</span> <span class="p">=</span> <span class="s">{15}</span>
<span class="p">}</span></code></pre></figure></div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2abbr"><abbr class="badge">WACV</abbr></div> <div id="zheltonozhskii2021c2d" class="col-sm-8"> <div class="title">Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels</div> <div class="author"> <em>EvgeniiZheltonozhskii</em>, <a href="https://chaimb.cs.technion.ac.il/" rel="external nofollow noopener" target="_blank">ChaimBaskin</a>, <a href="https://www.cs.technion.ac.il/~mendlson/" rel="external nofollow noopener" target="_blank">AviMendelson</a>, <a href="https://bron.cs.technion.ac.il/" rel="external nofollow noopener" target="_blank">Alex M.Bronstein</a>, and <a href="https://orlitany.github.io/" rel="external nofollow noopener" target="_blank">OrLitany</a> </div> <div class="periodical"> <em>In <span style="color:brown">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</span></em>, Jan 2022</div> <div class="periodical"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a><a href="https://openaccess.thecvf.com/content/WACV2022/html/Zheltonozhskii_Contrast_To_Divide_Self-Supervised_Pre-Training_for_Learning_With_Noisy_Labels_WACV_2022_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">URL</a><a href="https://arxiv.org/abs/2103.13646" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a><a href="https://github.com/ContrastToDivide/C2D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The success of learning with noisy labels (LNL) methods relies heavily on the success of a warm-up stage where standard supervised training is performed using the full (noisy) training set. In this paper, we identify a "warm-up obstacle": the inability of standard warm-up stages to train high quality feature extractors and avert memorization of noisy labels. We propose "Contrast to Divide" (C2D), a simple framework that solves this problem by pre-training the feature extractor in a self-supervised fashion. Using self-supervised pre-training boosts the performance of existing LNL approaches by drastically reducing the warm-up stage’s susceptibility to noise level, shortening its duration, and increasing extracted feature quality. C2D works out of the box with existing methods and demonstrates markedly improved performance, especially in the high noise regime, where we get a boost of more than 27% for CIFAR-100 with 90% noise over the previous state of the art. In real-life noise settings, C2D trained on mini-WebVision outperforms previous works both in WebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an in-depth analysis of the framework, including investigating the performance of different pre-training approaches and estimating the effective upper bound of the LNL performance with semi-supervised learning. Code for reproducing our experiments is available at this https URL</p> </div> <div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zheltonozhskii2021c2d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zheltonozhskii, Evgenii and Baskin, Chaim and Mendelson, Avi and Bronstein, Alex M. and Litany, Or}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1657--1667}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openaccess.thecvf.com/content/WACV2022/html/Zheltonozhskii_Contrast_To_Divide_Self-Supervised_Pre-Training_for_Learning_With_Noisy_Labels_WACV_2022_paper.html}</span><span class="p">,</span>
  <span class="na">sortkey</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure></div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2abbr"><abbr class="badge">NeurIPS <br> workshop</abbr></div> <div id="zheltonozhskii2020selfsupervised" class="col-sm-8"> <div class="title">Self-Supervised Learning for Large-Scale Unsupervised Image Clustering</div> <div class="author"> <em>EvgeniiZheltonozhskii</em>, <a href="https://chaimb.cs.technion.ac.il/" rel="external nofollow noopener" target="_blank">ChaimBaskin</a>, <a href="https://bron.cs.technion.ac.il/" rel="external nofollow noopener" target="_blank">Alex M.Bronstein</a>, and <a href="https://www.cs.technion.ac.il/~mendlson/" rel="external nofollow noopener" target="_blank">AviMendelson</a> </div> <div class="periodical">Aug 2020</div> <div class="periodical"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a><a href="https://arxiv.org/abs/2008.10312" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a><a href="https://github.com/Randl/kmeans_selfsuper" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at this https URL</p> </div> <div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zheltonozhskii2020selfsupervised</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Supervised Learning for Large-Scale Unsupervised Image Clustering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zheltonozhskii, Evgenii and Baskin, Chaim and Bronstein, Alex M. and Mendelson, Avi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeurIPS Self-Supervised Learning Workshop}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2008.10312}</span><span class="p">,</span>
  <span class="na">sortkey</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure></div> </div> </div> </li> </ol> </div> <div class="social-links"> <div class="contact-icons"> <a href="mailto:%7A%68%65%6C%74%6F%6E%6F%7A%68%73%6B%69%79@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://telegram.me/EvgeniyZh" title="telegram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-telegram"></i></a> <a href="https://orcid.org/0000-0002-5400-9321" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=1yHw4W0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Evgenii_Zheltonozhskii/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/Randl" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/zheltonozhskiy" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/evgeniyzhe" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a rel="me" href="https://mastodon.social/evgeniizh" title="Mastodon" target="_blank"><i class="fa-brands fa-mastodon"></i></a> </div> <div class="contact-note">You can always reach me by e-mail. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Evgenii Zheltonozhskii. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 13, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-VK1RZHGGZ9"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-VK1RZHGGZ9");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>