<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Self-Supervised Learning for Large-Scale Unsupervised Image Clustering | Evgenii Zheltonozhskii </title> <meta name="author" content="Evgenii Zheltonozhskii"> <meta name="description" content="Short summary of our latest paper"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?5e74696bc49f5dbb39f6c9843c020ed8"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://evgeniizh.com//blog/2020/unsupervised_ssl/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Evgenii </span> Zheltonozhskii </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Self-Supervised Learning for Large-Scale Unsupervised Image Clustering</h1> <p class="post-meta"> October 18, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this post I’ll summarize our latest paper, <a href="https://arxiv.org/abs/2008.10312" rel="external nofollow noopener" target="_blank">“Self-Supervised Learning for Large-Scale Unsupervised Image Clustering”</a>. In this work we found out that self-supervised learning is a strong baseline for fully unsupervised clustering of ImageNet (39% accuracy with 1000 clusters and 46% with 1500).</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Self-supervised learning is really hot now. In our new paper (<a href="https://t.co/n8pyI5CS3V" rel="external nofollow noopener" target="_blank">https://t.co/n8pyI5CS3V</a>) with <a href="https://twitter.com/ChaimBaskin?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@ChaimBaskin</a> Alex Bronstein and Avi Mendelson we study self-supervised learning in unsupervised clustering settings. The code is available at <a href="https://t.co/dGekFTW962" rel="external nofollow noopener" target="_blank">https://t.co/dGekFTW962</a> 1/n</p>— Evgenii Zheltonozhskii (@evgeniyzhe) <a href="https://twitter.com/evgeniyzhe/status/1298136113639497729?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">August 25, 2020</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Self-supervised learning became very popular recently, especially among large companies who have access to large amount of unlabelled data. In approximately last year, contrasitive losses have significantly boosed the performance of the self-supervised computer vision (you can get more details in blog posts in <a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html" rel="external nofollow noopener" target="_blank">English</a> or <a href="https://dyakonov.org/2020/06/03/%d1%81%d0%b0%d0%bc%d0%be%d0%be%d0%b1%d1%83%d1%87%d0%b5%d0%bd%d0%b8%d0%b5-self-supervision/" rel="external nofollow noopener" target="_blank">Russian</a>, which unfortunately age very fast).</p> <p>In particular, we try to review current state-of-the-art in the paper introduction. Lets me remind you how self-supervised approaches are usually evaluated. There are two main apporaches: either train a linear classifier on extracted features (some use k-NN instead), or fine-tune the network (usually on 1-10% of ImageNet, or on some segmentation dataset, e.g., COCO). Both have thier problem: first approach may fail if our features are good but not suitable for linear (or some other simple) classification; second one strongly depends on the way of fine tuning (cf. SimCLR v2). I was wondering for quite some time why nobody tried to employ a fully unsupervised pipeline to evaluate those methods. Since there appeared some attempts on unsupervised ImageNet clustering, I felt it is a good time to implement this idea. Since most of self-supervised methods publish both codes and models, we were able to perform extensive evaluation on relatively scarce compute.</p> <p>First of all, we needed to choose metrics, and evaluation of unsupervised learning is a hard task. Luckily, we have labels and thus we can calculate accuracy. In addition, we calculate some metrics based on pairwise accuracy (i.e., a pair is counted as positive if we managed to predict whether this pair belongs to the same cluster or to two different ones). We also adjust those metrics to random chance, acquiring two main metrics: adjusted Rand Index (ARI) and adjusted mutual information (AMI). We also added non-adjusted MI since it was used in some previous works.</p> <p>Since our goal is to create baseline, the approach is as simple as possible: we extract features from trainign and validation, apply PCA, and then train k-means. Linear assignment is used to calculate accuracy. This alone allows to achieve nearly SOTA results for unsupervised clustering. We also compared to top supervised CNNs. The results are highly correlated with linear classifier performance. One interesting exception is networks with high dimension of embedding space, which might be a shortcoming of k-means (or might be not).</p> <p>Moreover, we tested our approach on ObjectNet. Unfortunately, if trained on ImageNet no network performed better than random, even if we used validation set for label assignment (one notable exception is BigBiGAN). Moreover, supervised networks have similar problems.</p> <p>Nevertheless, if we train k-means on ObjectNet itself, we can get some non-trivial results (6.5% accuracy). Interestingly, there is no significant accuracy difference between classes include in ImageNet and ones that are not.</p> <p>Finally we performed small ablation study: checked the influence of dimentions of input (after PCA) and number of clusters (for less that 2000 clusters label-independent metrics increase).</p> <p>The paper provides some interesting insights and poses some questions which can not be answered that fast and require more fundamental work to answer.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tikzjax/">a post with TikZJax</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Evgenii Zheltonozhskii. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 13, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-VK1RZHGGZ9"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-VK1RZHGGZ9");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>